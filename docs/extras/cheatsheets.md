# ðŸ§¾ Cheatsheets â€” Quick Reference for AI Infrastructure Engineers

A condensed, at-a-glance guide to essential commands, architectures, and Azure resources for managing AI workloads efficiently.

---

## âš™ï¸ Azure GPU VM Comparison

| VM SKU | Ideal For | GPU Type | Inference | Training | Notes |
|--------|------------|-----------|------------|-----------|--------|
| **Standard_NC6s_v3** | General AI workloads | 1Ã— V100 | âœ… | âœ… (light) | Balanced price/performance |
| **Standard_NCas_T4_v3** | Cost-effective inference | 1Ã— T4 | âœ…âœ… | âŒ | Best option for production inference |
| **Standard_ND_A100_v4** | LLM training, deep learning | 1â€“8Ã— A100 | âœ…âœ…âœ… | âœ…âœ…âœ… | High cost, top-tier performance |
| **Standard_NVads_A10** | Visualization + lightweight AI | 1Ã— A10 | âœ… | âŒ | Ideal for dev/test workloads |

ðŸ’¡ *Tip:* For inference, prefer **T4 or A10 GPUs**. For large-scale training, use **A100s**.

---

## ðŸ“Š CPU vs GPU Quick Reference

| Attribute | CPU | GPU |
|------------|------|------|
| Cores | Tens | Thousands |
| Strength | General-purpose tasks | Parallel vector processing |
| Use Cases | Web apps, ETL, monitoring | Deep learning, embeddings, image processing |
| Azure Examples | Dv5, Ev4 | NCas_T4, ND_A100 |
| Cost | ðŸ’² | ðŸ’²ðŸ’²ðŸ’² |

ðŸ’¡ *Rule of thumb:* GPUs are specialized hardware. Use them only where parallelism matters.

---

## ðŸ” Security Checklist for AI Workloads

| Control | Description | Status |
|----------|--------------|---------|
| RBAC with Managed Identities | Assign least privilege roles | âœ… |
| Private Link for all endpoints | No public exposure of APIs | âœ… |
| Key Vault for secrets | No credentials in code or YAML | âœ… |
| Diagnostic logs enabled | Send to Log Analytics workspace | âœ… |
| API rate limiting configured | Prevent misuse and abuse | âœ… |
| Prompt injection testing done | Validate model input security | ðŸ”² |
| Data encryption (in transit & at rest) | TLS 1.2+ and SSE enabled | âœ… |

ðŸ’¡ *Tip:* Treat every model as a production API â€” with the same level of security scrutiny.

---

## ðŸ“ˆ Monitoring and Observability Cheat Sheet

| Metric | Source | Tool |
|--------|---------|------|
| GPU Utilization | `nvidia-smi`, DCGM Exporter | Prometheus / Grafana |
| Latency (P95) | Application Insights | Azure Monitor |
| Errors (429s, 5xx) | API logs | Application Gateway / App Insights |
| Cost per request | Log Analytics + Billing API | Power BI / Cost Management |
| Token usage | Azure OpenAI Metrics | Application Insights |

ðŸ’¡ *Tip:* Always correlate **GPU utilization + latency + token throughput**.

---

## ðŸ’¡ Infrastructure Quick Deploy Commands

### ðŸ§± Create a GPU VM
```bash
az vm create \
  --resource-group rg-ai-lab \
  --name vm-gpu \
  --image Ubuntu2204 \
  --size Standard_NC6s_v3 \
  --admin-username azureuser \
  --generate-ssh-keys
```

### ðŸš€ Deploy a Model Endpoint (Azure ML)
```bash
az ml online-endpoint create --name infer-demo --file endpoint.yml
```

### â˜ï¸ Create an AKS Cluster with GPU Node Pool (Terraform)
```hcl
resource "azurerm_kubernetes_cluster_node_pool" "gpu_pool" {
  name                  = "gpu"
  vm_size               = "Standard_NC6s_v3"
  enable_auto_scaling   = true
  min_count             = 1
  max_count             = 3
  node_labels = {
    "k8s.azure.com/mode" = "User"
  }
}
```

---

## ðŸ§  Performance and Throughput Formulas

| Metric | Formula | Example |
|--------|----------|----------|
| **TPM (Tokens per Minute)** | `(Input + Output Tokens) Ã— RPM` | (500 + 300) Ã— 1000 = 800,000 TPM |
| **QPS (Queries per Second)** | `RPM Ã· 60` | 300 RPM = 5 QPS |
| **Cost Estimation (Azure OpenAI)** | `Tokens Ã— Cost per 1K Tokens` | 100K tokens Ã— $0.002 = $0.20 |
| **GPU Scaling Efficiency** | `Active GPU Time Ã· Total Allocated Time` | 80% = efficient utilization |

ðŸ’¡ *Tip:* Log and analyze TPM, RPM, and QPS to prevent throttling and overprovisioning.

---

## ðŸ§° Where to Run Your Model â€” Decision Flow

```mermaid
graph TD
    A[Is your model already trained?] -->|Yes| B[Do you need scaling?]
    B -->|Yes| C[Use AKS + GPU Pool]
    B -->|No| D[Use single GPU VM]
    A -->|No| E[Use Azure ML for training pipeline]
```

ðŸ’¡ *Rule:*  
- **AKS + GPU Pool:** For scalable inference and production APIs  
- **VM:** For isolated testing or proof of concept  
- **Azure ML:** For training, registry, and lifecycle management

---

## ðŸ§­ Resource Tagging Convention for AI Workloads

| Tag | Example | Purpose |
|------|----------|----------|
| `Environment` | `dev`, `prod` | Identify lifecycle stage |
| `Team` | `AI-Infra` | Ownership |
| `CostCenter` | `12345` | Chargeback and budgeting |
| `Model` | `gpt4`, `vision-model-v2` | Correlate to API metrics |
| `Region` | `eastus`, `swedencentral` | Deployment region |

ðŸ’¡ *Tip:* Standardize tags for automation, budget tracking, and governance.

---

## ðŸ“˜ Reference Links

- [Azure Machine Learning Documentation](https://learn.microsoft.com/en-us/azure/machine-learning/)
- [Azure OpenAI Quotas & Limits](https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits)
- [Azure Kubernetes Service (AKS)](https://learn.microsoft.com/en-us/azure/aks/)
- [Azure Cost Management](https://learn.microsoft.com/en-us/azure/cost-management-billing/)
- [Azure Monitor for Containers](https://learn.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-overview)
