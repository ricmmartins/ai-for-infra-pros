# Technical FAQ: AI infrastructure essentials

A practical reference for infrastructure and cloud engineers adopting Artificial Intelligence in their environments.

---

## 1. Can I run AI workloads without a GPU?

**Yes, but with limitations.**

- Lightweight models (regression, decision trees, classical ML) can run efficiently on CPU.
- Large Language Models (LLMs), computer vision, and deep learning workloads require GPUs for acceptable latency and throughput.
- For costâ€‘effective inference on Azure, consider **Standard_NCas_T4_v3** or **NVads_A10** SKUs.

ðŸ’¡ *Tip:* Use **Spot VMs** only for nonâ€‘critical inference or batch jobs and always implement checkpointing and retries.

---

## 2. Whatâ€™s the difference between training and inference?

| Stage | Purpose | Infrastructure analogy |
|------|--------|------------------------|
| **Training** | Builds the model using historical data | Large batch job or offline benchmark |
| **Inference** | Executes the model for predictions | Stateless API responding to requests |

ðŸ’¡ *Infra lens:* Training is bursty and computeâ€‘heavy. Inference is latencyâ€‘sensitive and continuous.

---

## 3. How can I autoâ€‘scale AI workloads?

**Recommended signals:**

- GPU utilization and memory usage
- Request queue depth
- P95/P99 inference latency
- Tokens per minute (TPM) growth rate

**Azure implementations:**

- **AKS:** Cluster Autoscaler + HPA using Prometheus metrics
- **Azure ML:** `min_instances` / `max_instances` on online endpoints
- **VMSS:** Autoscale rules based on custom GPU metrics

ðŸ’¡ *Best practice:* Always configure cooldown periods to avoid scale oscillation.

---

## 4. How do I secure inference endpoints?

**Baseline controls:**

- Private Endpoints with VNet integration
- Authentication using **Entra ID (Azure AD)** where supported
- API keys only as a fallback, stored in **Azure Key Vault**
- WAF or API Management for throttling and abuse prevention
- Diagnostic logs enabled in **Application Insights**

ðŸ’¡ *Zero Trust rule:* Treat inference endpoints like any externalâ€‘facing production API.

---

## 5. How much does AI cost to run on Azure?

Costs vary by region, model, and usage profile.

| Resource | Typical cost range | Notes |
|--------|-------------------|------|
| Standard_NC6s_v3 | ~$1â€“$1.5/hr | Entry GPU workloads |
| Standard_NCas_T4_v3 | ~$0.8â€“$1/hr | Best inference cost efficiency |
| ND_A100_v4 | $25â€“$35/hr | Highâ€‘end training |
| Azure OpenAI (Standard) | Pay per token | Variable latency |
| Azure OpenAI (PTU) | Fixed hourly | Predictable throughput |

ðŸ’¡ *Tip:* Always pair GPU usage with **Azure Cost Management budgets and alerts**.

---

## 6. How do I monitor GPU usage and model latency?

**Key telemetry sources:**

- `nvidia-smi` for nodeâ€‘level checks
- **DCGM Exporter** for Prometheus
- **Azure Monitor for Containers**
- **Application Insights** for request latency, errors, and dependencies
- **Grafana** for unified dashboards

ðŸ’¡ *Golden rule:* Correlate GPU utilization, latency, and token throughput.

---

## 7. What are common bottlenecks in AI infrastructure?

| Area | Bottleneck | Mitigation |
|-----|-----------|-----------|
| Storage | Slow dataset reads | NVMe, Premium SSD |
| Network | Crossâ€‘region latency | Regional inference, Private Link |
| Compute | GPU idle time | Autoscaling, batching |
| Cost | Idle clusters | Scheduled shutdowns |

ðŸ’¡ *Reality:* The bottleneck is often the data path, not the GPU.

---

## 8. How do I estimate TPM, RPM, and cost for Azure OpenAI?

**Formula:**

```
TPM = Tokens per request Ã— Requests per minute
```

**Important considerations:**

- Tokens include both prompt and response.
- Retries amplify real TPM consumption.
- Monitor HTTP 429 and `Retry-After` headers.

ðŸ’¡ *Rule:* For sustained production traffic, evaluate **Provisioned Throughput Units (PTUs)**.

---

## 9. Whatâ€™s the best architecture for hybrid environments?

**Reference pattern:**

- Azure Arc for resource management
- Azure Monitor Agent for telemetry
- Private Link for secure connectivity
- Arcâ€‘enabled AKS for unified Kubernetes control

ðŸ’¡ *Design principle:* Keep inference close to the data to minimize latency and egress.

---

## 10. Whatâ€™s the best way to learn AI for infra engineers?

**Suggested path:**

1. AIâ€‘900: Azure AI Fundamentals
2. Handsâ€‘on labs (GPU VM, AKS GPU, Azure ML inference)
3. Azure OpenAI and monitoring deep dives
4. Build an internal Copilot or automation assistant

ðŸ’¡ *Mindset:* You donâ€™t need to become a data scientist. You need to master **how AI runs**.

---

> Infrastructure doesnâ€™t compete with AI.  
> It makes AI reliable, secure, and scalable.
